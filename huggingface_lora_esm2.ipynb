{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AmelieSchreiber/esm2_t6_8M_UR50D_lora_rna_binding_sites were not used when initializing EsmForTokenClassification: ['base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.esm.encoder.layer.0.output.dense.weight', 'base_model.model.esm.contact_head.regression.weight', 'base_model.model.esm.embeddings.position_ids', 'base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.esm.encoder.layer.0.attention.self.query.bias', 'base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.4.attention.self.key.bias', 'base_model.model.esm.encoder.layer.1.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.4.output.dense.bias', 'base_model.model.esm.encoder.layer.3.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.3.attention.LayerNorm.bias', 'base_model.model.esm.encoder.layer.4.attention.self.query.weight', 'base_model.model.esm.encoder.layer.1.attention.self.key.bias', 'base_model.model.esm.encoder.layer.3.attention.self.query.bias', 'base_model.model.esm.encoder.layer.0.attention.self.key.weight', 'base_model.model.esm.encoder.layer.4.output.dense.weight', 'base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.esm.encoder.layer.5.attention.self.key.weight', 'base_model.model.esm.encoder.layer.5.output.dense.weight', 'base_model.model.esm.encoder.layer.3.LayerNorm.weight', 'base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.attention.self.query.weight', 'base_model.model.esm.encoder.layer.1.attention.self.key.weight', 'base_model.model.esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.0.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.0.output.dense.bias', 'base_model.model.esm.encoder.layer.1.LayerNorm.bias', 'base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.3.LayerNorm.bias', 'base_model.model.esm.encoder.layer.4.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.5.LayerNorm.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.esm.encoder.layer.3.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.4.LayerNorm.bias', 'base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.esm.encoder.layer.1.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.3.attention.self.query.weight', 'base_model.model.esm.encoder.layer.1.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.esm.encoder.layer.1.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.2.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.esm.encoder.layer.1.output.dense.weight', 'base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.4.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.esm.encoder.layer.5.intermediate.dense.bias', 'base_model.model.esm.encoder.layer.0.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.5.attention.self.query.weight', 'base_model.model.esm.encoder.layer.5.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.4.attention.LayerNorm.bias', 'base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.esm.encoder.layer.2.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.0.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.3.attention.self.value.weight', 'base_model.model.esm.encoder.layer.3.output.dense.bias', 'base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.esm.encoder.layer.1.attention.self.query.bias', 'base_model.model.esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.5.attention.self.value.weight', 'base_model.model.esm.encoder.layer.2.LayerNorm.bias', 'base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.esm.encoder.layer.3.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.1.intermediate.dense.bias', 'base_model.model.esm.embeddings.position_embeddings.weight', 'base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.1.attention.self.query.weight', 'base_model.model.esm.encoder.layer.1.output.dense.bias', 'base_model.model.esm.encoder.layer.0.LayerNorm.weight', 'base_model.model.esm.encoder.layer.5.LayerNorm.bias', 'base_model.model.esm.encoder.emb_layer_norm_after.bias', 'base_model.model.esm.encoder.emb_layer_norm_after.weight', 'base_model.model.esm.encoder.layer.0.attention.self.key.bias', 'base_model.model.esm.encoder.layer.0.attention.self.value.bias', 'base_model.model.esm.encoder.layer.4.attention.self.value.bias', 'base_model.model.esm.encoder.layer.1.attention.self.value.bias', 'base_model.model.esm.encoder.layer.1.LayerNorm.weight', 'base_model.model.esm.encoder.layer.4.LayerNorm.weight', 'base_model.model.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.2.attention.self.key.weight', 'base_model.model.esm.encoder.layer.0.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.1.attention.LayerNorm.bias', 'base_model.model.esm.encoder.layer.2.attention.self.key.bias', 'base_model.model.esm.encoder.layer.3.output.dense.weight', 'base_model.model.esm.encoder.layer.2.attention.self.value.weight', 'base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.intermediate.dense.bias', 'base_model.model.esm.encoder.layer.0.attention.self.value.weight', 'base_model.model.esm.encoder.layer.4.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.4.attention.self.value.weight', 'base_model.model.esm.encoder.layer.2.attention.self.query.bias', 'base_model.model.esm.encoder.layer.4.intermediate.dense.bias', 'base_model.model.esm.encoder.layer.1.attention.self.value.weight', 'base_model.model.esm.encoder.layer.5.attention.LayerNorm.weight', 'base_model.model.esm.encoder.layer.2.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.2.attention.self.value.bias', 'base_model.model.esm.encoder.layer.4.attention.self.query.bias', 'base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.4.attention.output.dense.weight', 'base_model.model.esm.encoder.layer.5.attention.LayerNorm.bias', 'base_model.model.esm.encoder.layer.4.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.esm.encoder.layer.5.attention.self.value.bias', 'base_model.model.esm.encoder.layer.5.attention.output.dense.bias', 'base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.esm.encoder.layer.5.output.dense.bias', 'base_model.model.esm.encoder.layer.2.attention.LayerNorm.bias', 'base_model.model.esm.contact_head.regression.bias', 'base_model.model.esm.encoder.layer.3.intermediate.dense.bias', 'base_model.model.esm.encoder.layer.3.attention.self.key.weight', 'base_model.model.esm.encoder.layer.0.attention.LayerNorm.bias', 'base_model.model.esm.encoder.layer.0.intermediate.dense.bias', 'base_model.model.esm.encoder.layer.3.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.esm.embeddings.word_embeddings.weight', 'base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.3.attention.self.key.bias', 'base_model.model.esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq', 'base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.2.output.dense.weight', 'base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.esm.encoder.layer.3.attention.self.value.bias', 'base_model.model.esm.encoder.layer.5.attention.self.key.bias', 'base_model.model.esm.encoder.layer.2.LayerNorm.weight', 'base_model.model.esm.encoder.layer.0.LayerNorm.bias', 'base_model.model.esm.encoder.layer.5.intermediate.dense.weight', 'base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.esm.encoder.layer.5.attention.self.query.bias', 'base_model.model.esm.encoder.layer.0.attention.self.query.weight', 'base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight']\n",
      "- This IS expected if you are initializing EsmForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at AmelieSchreiber/esm2_t6_8M_UR50D_lora_rna_binding_sites and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[[ 0.0193, -0.6626],\n",
      "         [-0.1956, -0.7331],\n",
      "         [-0.1698, -0.7026],\n",
      "         ...,\n",
      "         [-0.1562, -0.7934],\n",
      "         [-0.1561, -0.7931],\n",
      "         [-0.1559, -0.7933]]])\n",
      "Predicted binding sites: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"AmelieSchreiber/esm2_t6_8M_UR50D_lora_rna_binding_sites\"\n",
    "\n",
    "# Load the model\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# New unseen protein sequence\n",
    "new_protein_sequence = \"FDLNDFLEQKVLVRMEAIINSMTMKERAKPEIIKGSRKRRIAAGSGMQVQDVNRLLKQFDDMQRMMKKM\"\n",
    "\n",
    "# Tokenize the new sequence\n",
    "inputs = loaded_tokenizer(new_protein_sequence, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Print logits for debugging\n",
    "print(\"Logits:\", logits)\n",
    "\n",
    "# Convert predictions to a list\n",
    "predicted_labels = predictions.squeeze().tolist()\n",
    "\n",
    "# Get input IDs to identify padding and special tokens\n",
    "input_ids = inputs['input_ids'].squeeze().tolist()\n",
    "\n",
    "# Define a set of token IDs that correspond to special tokens\n",
    "special_tokens_ids = {loaded_tokenizer.cls_token_id, loaded_tokenizer.pad_token_id, loaded_tokenizer.eos_token_id}\n",
    "\n",
    "# Filter the predicted labels using the special_tokens_ids to remove predictions for special tokens\n",
    "binding_sites = [label for label, token_id in zip(predicted_labels, input_ids) if token_id not in special_tokens_ids]\n",
    "\n",
    "print(\"Predicted binding sites:\", binding_sites)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_esm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
